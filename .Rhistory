library(shiny); runApp('TReK/ORKGcsv.r')
library(shiny); runApp('TReK/ORKGcsv.r')
runApp('TReK/ORKGcsv.r')
runApp('TReK/ORKGcsv.r')
# Many thanks to this amazing tutorial: https://kateto.net/network-visualization
# Load required libraries
library(xml2)
library(igraph)
library(plotly)
library(XML)
library(dplyr)
library(stringr)
library(tidyr)
library(visNetwork)
# Download and read the RDF/XML file
url <- "http://np.knowledgepixels.com/RAzquSkwsTAZm61nReG6MOjXEXUx8fNVfdWnAzyn6sOhU.xml"  # Replace with your actual URL
name <- "RAzquSkwsT"
xml_data <- read_xml(url)
# Function to get a human-readable label
get_label <- function(uri) {
# There's no labels in nanopubs so we just process the URI here.
#Basically this string checks if it has a nice label (indicated by #)
#If not it returns the tail of the URL which tends to be the ontology URI
label_tails <- str_extract(uri, "/([^/]+)$") %>% str_remove("^/")
label_neat <- str_extract(label_tails, "#([^#]+)$") %>% str_remove("^#")
label <- coalesce(label_neat, label_tails)
return(label)
}
# Extract nodes and edges
# The XML namespace needs to be referenced in the function:
# Find all graph elements
graphs <- xml_find_all(xml_data, ".//d1:graph", xml_ns(xml_data))
# Function to process a single graph into dataframe
process_graph <- function(graph) {
graph_uri <- xml_text(xml_find_first(graph, "./d1:uri"))
triples <- xml_find_all(graph, ".//d1:triple")
triples %>%
lapply(function(triple) {
uris <- xml_find_all(triple, ".//d1:uri") %>% xml_text()
data.frame(
graph = graph_uri,
subject = uris[1],
predicate = uris[2],
object = uris[3],
stringsAsFactors = FALSE
)
}) %>%
bind_rows()
}
# Process all graphs and combine results
df <- graphs %>%
lapply(process_graph) %>%
bind_rows()
# Print the resulting dataframe for debugging
print(df)
#Here we're going to strip the messy URI from graph
#This regex just pulls the end of the URI which is usually the label
df <- df %>%
mutate(graph_labels = get_label(df$graph)) %>%
mutate(object_labels = get_label(df$object)) %>%
mutate(predicate_labels = get_label(df$predicate)) %>%
mutate(subject_labels = get_label(df$subject))
#todo - split up nodes including graph label info
# - create edges table
# - stylize graph
#We need to reorder the df to be subject & object, then predicate.
#We also need to create a separate vertex metadata df for labels
#This is because igraph processes the first two columns as edgelist
edges <- df[, c("subject", "object", "predicate", "predicate_labels")]
#We drop any edges with NA in them.
edges <- drop_na(edges, c("subject", "object"))
#We then need to pull out all unique vertices from subject and object and their
#associated metadata
# Create two dataframes with unique values from column1 and column2
temp1 <- df %>%
select(subject, graph_labels, subject_labels) %>%
distinct(subject, .keep_all = TRUE)
temp2 <- df %>%
select(object, graph_labels, object_labels) %>%
distinct(object, .keep_all = TRUE)
# Combine the two dataframes
merged_vertices <- bind_rows(
temp1 %>% rename(uri = subject),
temp2 %>% rename(uri = object)
) %>%
distinct(uri, .keep_all = TRUE)
merged_vertices <- drop_na(merged_vertices, "uri")
# We need to merge the labels into one label column.
merged_vertices <- merged_vertices %>% unite("label", c("subject_labels", "object_labels"), na.rm = TRUE, remove = FALSE)
#Let's change each graph into a colour so we can split things up.
merged_vertices <- merged_vertices %>% mutate(colour = recode(graph_labels, Head = "blue", Assertion = "green", Provenance = "orange", Pubinfo = "grey"))
clean_vertices <- merged_vertices[,c("uri", "label", "graph_labels")]
clean_vertices <- rename(clean_vertices, clean_label = label)
#NOW we add our edges and vertices and make an igraph object
graph <- graph_from_data_frame(edges, directed = TRUE, vertices = clean_vertices)
#Let's convert for visNetwork which is a nicer visualization
g_vis <- toVisNetworkData(graph)
vis.nodes <- g_vis$nodes
vis.edges <- g_vis$edges
vis.nodes$shape <- "dot"
vis.nodes$shadow <- TRUE # Nodes will drop shadow
vis.nodes$title  <- vis.nodes$uri # Text on click
vis.nodes$label <- vis.nodes$clean_label
vis.nodes$borderWidth <- 2 # Node border width
vis.nodes$color <- vis.nodes$colour
vis.nodes$group <- vis.nodes$graph_labels
vis.edges$title <- vis.edges$predicate_labels
vis.edges$label <- vis.edges$predicate_labels
#TODO: Make the below code turn nodes into links when clicked
visNetwork(vis.nodes, vis.edges) %>%
visGroups(groupname = "Head", color = "grey") %>%
visGroups(groupname = "Assertion", color = "green") %>%
visGroups(groupname = "Pubinfo", color = "purple") %>%
visGroups(groupname = "Provenance", color = "orange") %>%
visEvents(doubleClick = "function(properties) {
window.open(properties.nodes)
}") %>%
visLegend() %>%
visSave("vis.html")
getwd()
library(rjson)
install.packages("rjson")
library(rjson)
?rjson
?`rjson-package`
# Download and read the RDF/XML file
url <- "https://np.knowledgepixels.com/RAzquSkwsTAZm61nReG6MOjXEXUx8fNVfdWnAzyn6sOhU.jsonld"  # Replace with your actual URL
name <- "RAzquSkwsT"
json_data <- fromJSON(url, method = "C", simplify = TRUE)
?fromJSON
json_data <- fromJSON(file = url, method = "C", simplify = TRUE)
View(json_data)
library(csv)
library(tidyverse)
df <- read.csv("C:\Users\Tim Alamenciak\Downloads\10-sample-papers-OntoGPT---ORKG.csv")
df <- read.csv("C:/Users/Tim Alamenciak/Downloads/10-sample-papers-OntoGPT---ORKG.csv")
View(df)
df <- read.csv("C:/Users/Tim Alamenciak/Documents/Coding/OntoGPTtoORKG9.csv")
View(df)
df <- read.csv("C:/Users/Tim Alamenciak/Documents/Coding/OntoGPTtoORKG9.csv")
View(df)
df <- read.csv("C:/Users/Tim Alamenciak/Documents/Coding/OntoGPTtoORKG9.csv")
View(df)
library(orkg)
install.packages("orkg")
install.packages("orkg-r")
devtools::install_gitlab("TIBHannover/orkg/orkg-r")
install.packages("devtools")
devtools::install_gitlab("TIBHannover/orkg/orkg-r")
library(orkg)
?orgk
?orkg
??orkg
View(df)
df <- read.csv("C:/Users/Tim Alamenciak/Documents/Coding/OntoGPTtoORKG9.csv")
View(df)
df <- group_by(df, "File")
#Step 1: Merge all CSV's in openrefine
#Step 2: Use Excel to manually paste over the DOIs
#Step 3: delete dois, ID and label
library(tidyverse)
df <- group_by(df, "File")
summary(df)
flatten <- function(x) {
print(x)
}
group_map(df, flatten)
group_map(df, ~ flatten)
group_map(df, flatten(x))
library(tidyverse)
library(visNetwork)
#Going to need a dataframe with $nodes and $edges
#Edge types: increases, decreases, stable
nodes <- data.frame(nodes_id = c(1,2,3,4,5),
nodes = c("Fruit/foliage",
"Kokako",
"Ship rats",
"Possums",
"Stoats"))
nodes
getwd()
edges <- read.csv("Coding/test_edges.csv")
View(edges)
vis.nodes <- nodes
vis.nodes <- nodes
vis.edges <- edges
visNetwork(vis.nodes, vis.edges)
?visNetwork
View(edges)
#Going to need a dataframe with $nodes and $edges
#Edge types: increases, decreases, stable
nodes <- data.frame(id = c(1,2,3,4,5),
nodes = c("Fruit/foliage",
"Kokako",
"Ship rats",
"Possums",
"Stoats"),
selfdamping = c("N","N","Y","Y","Y"))
edges <- read.csv("Coding/test_edges.csv")
vis.nodes <- nodes
vis.edges <- edges
visNetwork(vis.nodes, vis.edges)
?visNetwork
visNetwork(vis.nodes, vis.edges) %>%
visNodes(title = vis.nodes$nodes)
View(vis.nodes)
?visNodes
visNetwork(vis.nodes, vis.edges) %>%
visNodes(label = nodes)
visNetwork(vis.nodes, vis.edges) %>%
visNodes(label = "nodes")
visNetwork(vis.nodes, vis.edges) %>%
visNodes(label = vis.nodes$nodes)
?visNodes
?visNetwork
#Going to need a dataframe with $nodes and $edges
#Edge types: increases, decreases, stable
nodes <- data.frame(id = c(1,2,3,4,5),
label = c("Fruit/foliage",
"Kokako",
"Ship rats",
"Possums",
"Stoats"),
selfdamping = c("N","N","Y","Y","Y"))
vis.nodes <- nodes
vis.edges <- edges
visNetwork(vis.nodes, vis.edges) %>%
visNodes(label = vis.nodes$nodes)
?visEdges
?visNetwork
visNetwork(vis.nodes, vis.edges) %>%
visNodes(label = vis.nodes$nodes) %>%
visEdges(arrows = to)
visNetwork(vis.nodes, vis.edges) %>%
visNodes(label = vis.nodes$nodes) %>%
visEdges(arrows = "to")
?visEdges
visDocumentation()
visDocumentation()
visNetwork(vis.nodes, vis.edges) %>%
visNodes(label = vis.nodes$nodes) %>%
visEdges(arrows = "to", arrows.to.type="circle")
edges <- read.csv("Coding/test_edges.csv")
vis.edges <- edges
visNetwork(vis.nodes, vis.edges) %>%
visNodes(label = vis.nodes$nodes) %>%
visEdges(arrows = "to", arrows.to.type="circle")
visNetwork(vis.nodes, vis.edges) %>%
visNodes(label = vis.nodes$nodes) %>%
visEdges(arrows = "to")
?diagrammer
library(DiagrammeR)
?DiagrammeR
DiagrammeR("graph LR")
DiagrammeR("graph LR 1-->2")
?DiagrammeR
DiagrammeR("
graph LR
A-->B
A-->C
C-->E
B-->D
C-->D
D-->F
E-->F
")
#Might be easier to use DiagrammeR
DiagrammeR("
graph LR
A--*B
A-->C
C-->E
B-->D
C-->D
D-->F
E-->F
")
#Might be easier to use DiagrammeR
DiagrammeR("
graph LR
A--]B
A-->C
C-->E
B-->D
C-->D
D-->F
E-->F
")
#Might be easier to use DiagrammeR
DiagrammeR("
graph LR
A-->B
A-->C
C-->E
B-->D
C-->D
D-->F
E-->F
")
?diagrammer
?DiagrammeR
DiagrammeR("graph LR;A(Rounded)-->B[Squared];B-->C{A Decision};
C-->D[Square One];C-->E[Square Two];
style A fill:#E5E25F;  style B fill:#87AB51; style C fill:#3C8937;
style D fill:#23772C;  style E fill:#B6E6E6;"
)
?DiagrammeR
?graph
?grVizOutput
?add_edge
library(fcm)
install.packages("fcm")
library(shiny); runApp('Coding/KGMaker.R')
runApp('Coding/KGMaker.R')
?updateSelectInput
runApp('Coding/KGMaker.R')
runApp('Coding/KGMaker.R')
runApp('Coding/KGMaker.R')
runApp('Coding/KGMaker.R')
runApp('Coding/KGMaker.R')
runApp('Coding/KGMaker.R')
runApp('Coding/KGMaker.R')
?add_edge
runApp('Coding/KGMaker.R')
?set_edge_attrs
runApp('Coding/KGMaker.R')
?set_edge_attrs
runApp('Coding/KGMaker.R')
runApp('Coding/KGMaker.R')
runApp('Coding/KGMaker.R')
runApp('Coding/KGMaker.R')
?set_edge_attrs
runApp('Coding/KGMaker.R')
library(SPARQL)
install.packages("SPARQL")
install.packages("C:/Users/Tim Alamenciak/Downloads/SPARQL_1.16.tar.gz", repos = NULL, type = "source")
install.packages("RCurl")
install.packages("C:/Users/Tim Alamenciak/Downloads/SPARQL_1.16.tar.gz", repos = NULL, type = "source")
library(SPARQL)
?SPARQL
SPARQL(url = "https://zif-sandbox.wikibase.cloud/query/", query = query)
query = "SELECT * WHERE { ?s ?p ?o . } LIMIT 10"
SPARQL(url = "https://zif-sandbox.wikibase.cloud/query/", query = query)
SPARQL(url = "https://zif-sandbox.wikibase.cloud/query/", query = query)
library(SPARQL)
SPARQL(url = "https://zif-sandbox.wikibase.cloud/query/", query = query)
library(SPARQLchunks)
knitr::knit_engines$set(sparql = SPARQLchunks::eng_sparql)
?SPARQLchunks
??SPARQLchunks
?sparql2df
sparql2df(endpoint, query)
endpoint = "https://zif-sandbox.wikibase.cloud/query/"
endpoint = "https://zif-sandbox.wikibase.cloud/query/"
sparql2df(endpoint, query)
test <- sparql2df(endpoint, query)
View(test)
endpoint = "https://query.wikidata.org/sparql"
sparql2df(endpoint, query)
endpoint = "https://zif-sandbox.wikibase.cloud/query"
sparql2df(endpoint, query)
endpoint = "https://beyond-notability.wikibase.cloud/query/"
sparql2df(endpoint, query)
endpoint = "https://zif-sandbox.wikibase.cloud/query#"
sparql2df(endpoint, query)
endpoint = "http://zif-sandbox.wikibase.cloud/query"
sparql2df(endpoint, query)
endpoint = "https://lindas.admin.ch/query"
sparql2df(endpoint, query)
endpoint = "https://zif-sandbox.wikibase.cloud/query/"
sparql2df(endpoint, query)
endpoint = "https://zif-sandbox.wikibase.cloud/query/sparql"
sparql2df(endpoint, query)
library(shiny); runApp('Coding/KGMaker.R')
runApp('Coding/KGMaker.R')
library(fcmapper)
install.packages("FCMapper")
library(fcmapper)
library(FCMapper)
?FCMapper
changes.scenario
combine.maps
library(shiny); runApp('Coding/KGMaker.R')
library(shiny); runApp('Coding/KGMaker.R')
o$this <- test
o <- data.frame()
o$this <- test
o$this <- "test"
View(o)
o <- data.frame()
o$this <- c(1,2,3)
o <- data.frame("test" = 1, "this" = 2)
View(o)
o$this
o$not
isnull(o$not)
is.null(o$not)
!is.null(o$convergence)
!is.null(o$convergence)
?req
??req
getwd()
setwd("C:/Users/Tim Alamenciak/Documents/Coding/Futzy")
deployApp()
library(rsconnect)
deployApp()
sigmoid(-0.6)
library(sigmoid)
install.packages("sigmoid")
library(sigmoid)
sigmoid(-0.6)
sigmoid(-0.3)
sigmoid(-0.6)
tanh(-0.6)
library(fcm)
?fcm.infer
